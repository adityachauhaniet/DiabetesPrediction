{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5P0EYWtbtJnypN0p+7n95",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityachauhaniet/Fake-News-Prediction/blob/main/M4_6_Project1_Diabetes_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Diabetes Prediction Using ML with Python**\n"
      ],
      "metadata": {
        "id": "bf-6e2bBZZam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Dependencies"
      ],
      "metadata": {
        "id": "qilSwcptZpLv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx3cGDerZHi-"
      },
      "outputs": [],
      "source": [
        "import numpy as np # for craeting arrays\n",
        "import pandas as pd #to create dataframe to analyse the data\n",
        "from sklearn.preprocessing import StandardScaler #for standardize the data\n",
        "from sklearn.model_selection import train_test_split #forspliting the dataset into tarining and test data\n",
        "from sklearn import svm #for training the model\n",
        "from sklearn.metrics import accuracy_score #to check the model accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Collection & Analysis"
      ],
      "metadata": {
        "id": "8VJMjTdIbyLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#first we exctract the dataset, b/c dataset is downloaded in zip form if dataset in zipform\n",
        "#from zipfile import ZipFile\n",
        "#dataset = '/content/diabetes.zip'\n",
        "#with ZipFile(dataset,'r') as zip: #r-> read the zip as zip\n",
        " # zip.extractall()\n",
        "  #print(\"The dataset is extracted\")"
      ],
      "metadata": {
        "id": "XMwHuYrlhgOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the diabetes dataset to a pandas dataframe\n",
        "diabetes_dataset = pd.read_csv('/content/diabetes.csv')"
      ],
      "metadata": {
        "id": "6WuLtsAmbRY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_dataset.head()"
      ],
      "metadata": {
        "id": "qK-VcJiVjNNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_dataset.shape"
      ],
      "metadata": {
        "id": "hmuUvFsxkA8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#some statistical measure of this data frame\n",
        "#getting statistical measure(mean, S.D. percentage etc) of the data\n",
        "diabetes_dataset.describe()"
      ],
      "metadata": {
        "id": "4E_6_GSel94c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets see how many cases of diabetes or non diabetes\n",
        "diabetes_dataset['Outcome'].value_counts() # Outcome column show 0-> non diabetic, 1->diabetic"
      ],
      "metadata": {
        "id": "RVM8Co2Jmfkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get mean values for all value of 0 & 1\n",
        "diabetes_dataset.groupby('Outcome').mean()"
      ],
      "metadata": {
        "id": "U6p8UTqtnLts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in above outpur the feature are Pregnencies, Glucose, BloodPressure etc. Let Glucose has mean value for non diabetic => 109 and diabetic => 141, it means all diabetic patient has glucose value 141 , its avaeage value"
      ],
      "metadata": {
        "id": "6GmzXS2Bn3nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets seperate the data and labels(0,1)\n",
        "#$create variable called x, and store all trhe dat6a except labels\n",
        "x = diabetes_dataset.drop(columns = 'Outcome', axis = 1) # we are taking x & droping the particular column(Outcome)\n",
        "# & you need to specify axis=1. If axis=0, it means you are droping the particular row not column\n",
        "# now storing all thoes labels in the variable y\n",
        "y = diabetes_dataset['Outcome']"
      ],
      "metadata": {
        "id": "wu_o_zQCopjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "Q-dzO876r-qC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stanadardized the Data"
      ],
      "metadata": {
        "id": "dYy9LlyosH-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Why we are doing this, if we see in dataset like Glucose(148,85,183,---,89). There are difference in range\n",
        "# and it is difficult for ML Models to make some prediction\n",
        "# We standardized the data in a particular range to helps our ML Model to make better prediction\n",
        "# We already imported a fucn (StanadrScaler) to this purpose\n",
        "#create a var scaler and load that fucn"
      ],
      "metadata": {
        "id": "aT0dac97sDaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()"
      ],
      "metadata": {
        "id": "6Q7eQ4ULtSHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to fit the data in this scaler variable as this StandardScaler fucn\n",
        "scaler.fit(x)"
      ],
      "metadata": {
        "id": "fnL-5g1ltXur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we need to transform this data, create another var standardized_data\n",
        "standardized_data = scaler.transform(x)"
      ],
      "metadata": {
        "id": "VZL7ED7LtoVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so basicalyy we fiting the data with out standardcsaler fucn, base on stanadardization we transfgorm all th data to a common range\n",
        "# print data in standardized data\n",
        "print(standardized_data)"
      ],
      "metadata": {
        "id": "GaCJPVnnuMgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can see that all the data lie in a range(0 & 1)\n",
        "# lets given this standardized data to variable x"
      ],
      "metadata": {
        "id": "ferWdiyhu6Mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = standardized_data"
      ],
      "metadata": {
        "id": "5uUdMp4evZjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# and y is again labels\n",
        "y = diabetes_dataset['Outcome']"
      ],
      "metadata": {
        "id": "XJAkg3uzviAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "m2xjIzZwvv5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we see the changes in data\n",
        "# the next steps is spliting the data into two part, One is training data anad second is test data"
      ],
      "metadata": {
        "id": "mQZBQJ6-vzoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Test Split"
      ],
      "metadata": {
        "id": "uNqdqKuWwFUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to mention 4 valiable and use train_test_split fucn to spliting the data\n",
        "x_train, x_test, y_train, y_test= train_test_split(x,y, test_size = 0.2, stratify = y, random_state = 2)"
      ],
      "metadata": {
        "id": "Rtd-FT2owHxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x-> split into x_train, x_test. x_train--> is used to train the model, and once the model is trained the evaluat the model with x_test data.\n",
        "# y_train--> it contain all the labels for x_train data, y_test--> represent all the labels which is(0 or 1) for the x_test data\n",
        "# test_size = 0.2 --> it means the size of test data is 20% of original dataset(0.2-->20%), and remaining 80% data is for training data\n",
        "# stratify = y-->(basically stratifying based on y) means y as the value either 0 or 1, so we want to split dataset in the same proportion.Like\n",
        "# like: if we don't mention this then there is chance that all diabetic data cases goes into x_train, and all non diabetic data cases goes into x_test.\n",
        "# random_state =2--> random_state helps to spliting the data in a particular way. Here random_state=2, it means lets say you are makin the similar code & want to spliting the datain a same way I did so, in that case you need to mention 2,\n",
        "#If we mention random_state = 1--> then it means you want to spliting the data in different way"
      ],
      "metadata": {
        "id": "lR1AVD79xTgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.shape, x_train.shape, x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF11IfaiiXCZ",
        "outputId": "27fe3575-5d2c-4a0a-a2ee-722764acaf81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(768, 8) (614, 8) (154, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we are going to trained the model"
      ],
      "metadata": {
        "id": "8ItB34faiqtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Model"
      ],
      "metadata": {
        "id": "muBd0BoriwuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = svm.SVC(kernel ='linear')"
      ],
      "metadata": {
        "id": "Rt3wSqdzwJQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we are going to use svm fucn to load our support vector machine & svc-->support vector classifier and another parameter kernel--> kernel= linear so we going t use a linear model\n",
        "# Now will fill the training data to this classifier\n",
        "#training the support vector machine classifier & the training data x_train & hie label y_train"
      ],
      "metadata": {
        "id": "JHDy4ABhwJ4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.fit(x_train, y_train) #this are trained our ML Model"
      ],
      "metadata": {
        "id": "7PrT4OHWx0EC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Till now our ML Model have been trained\n",
        "# Now we can evaluate our model\n",
        "#So basically our evaluation is how many time our model is predicting correctly"
      ],
      "metadata": {
        "id": "SF5l0IIlx-oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation :\n",
        "      Accuracy Score"
      ],
      "metadata": {
        "id": "nmzXV4Ghydiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#So we first lets try to find accuracy score on training data\n",
        "#SO in this process, w'll predict all these training data due to this we won't give the labels to the model, b/c we predicting the labels on the basis of training data\n",
        "#& w'll compare that prediction of our model to  the original labels which is y_train & try to predict accuracy score"
      ],
      "metadata": {
        "id": "IrfskjKzylzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy score on the training data\n",
        "x_train_prediction = classifier.predict(x_train)"
      ],
      "metadata": {
        "id": "x1eLB9x6yz4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#what we doing, basically we prdeicting the labels for all the x_train & storing all the labels in this x_train_predictiopn variable\n",
        "#Now we need to find training data accuracy, and use the fucn(accuracy_score) to this purpose which is earlier imported"
      ],
      "metadata": {
        "id": "u1WMVO1oz64w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data_accuarcy = accuracy_score(x_train_prediction, y_train) # it is compare the both labels,the model predicted labels(x_train_prediction) and original data labels(y_train)\n"
      ],
      "metadata": {
        "id": "S3tHdhHz0dHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#So what we are doing, we are using our trained ML Model so once we fit our ML Model model is stored in the var classifier.\n",
        "#Now we are using that model to predict the labels\n",
        "#and we predicting all the labels for x_train, storing in x_train_prediction and comparing with original labels that is y_train\n",
        "#so this will give us accuracy score of our model"
      ],
      "metadata": {
        "id": "DYyvWOoo06F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy score of the training data :', training_data_accuarcy)"
      ],
      "metadata": {
        "id": "AX1c2aKn2ceu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#here we can see in the above output we accuracy score is 78, if the score is greator than  75 it consider in pretty good, But in this traing process we are using very less size of data\n",
        "#Now let's try to find accuracy score on test data"
      ],
      "metadata": {
        "id": "wcSpiiUL2nmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy score on test data\n",
        "x_test_prediction = classifier.predict(x_test) # we are storing all prediction for test data in this variable\n",
        "test_data_accuracy = accuracy_score(x_test_prediction, y_test) # in this we comparing the both labels(x_test_predictio & y_test) and calculating accuracy score and stored cin test_data_accuracy"
      ],
      "metadata": {
        "id": "wY6z_38V3M8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's print the score of test data\n",
        "print('Accuracy score of test data :',test_data_accuracy)"
      ],
      "metadata": {
        "id": "MTTfI-an4XbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can noticed that the score of training data is 78  and score of test data is 77, it is good evidence that the model has not overtrained\n",
        "#overtraining represent the modle just trains a lot on the training data that it cannot perform well on the test data so in that case the training data accuracy w'll be very high and tst data accuracy w'll be very low\n",
        "# and this concept is known as overfiting"
      ],
      "metadata": {
        "id": "JDoGN4oj4tuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we nee dto make prediction system that can be predict whether a person has diabetic or not given on thoes data"
      ],
      "metadata": {
        "id": "eLdha_1D5p3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making Prediction System"
      ],
      "metadata": {
        "id": "lou4SboO6FZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Array input data, we need to  give all thoes medical information\n",
        "input_data = (4,110,92,0,0,37.6,0.191,30) # this data is copied from diabetes.csv dataset file and in the dataset this particular data has label is = 0, lets check model prediction\n",
        "#before the run this and make prediction , first we need to  change this input_data list to numpy array"
      ],
      "metadata": {
        "id": "7rxpdt7x6KpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data_as_numpy_array = np.asarray(input_data)"
      ],
      "metadata": {
        "id": "6zC5jyjh9aPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#and now we need to reshape this data\n",
        "#reshape the array as we are predicting for one instance\n",
        "#why this? --> our model is trainde for 768 examples& totally 8 columns but in this case we use one data points\n",
        "# if we don't reshape array what the model expects, it is expects 768 values or datapoints, but we rea using just one, so this will make confusion to the model"
      ],
      "metadata": {
        "id": "h_B-qgkl7weK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)"
      ],
      "metadata": {
        "id": "1JXsPwcS89E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#there is one more important thing we cannot give the data (4,110,92,0,0,37.6,0.191,30) as such, b/c we trained our model on standardized data, so we need to first standardized the input data\n",
        "#stanadrdized the input data\n",
        "std_data = scaler.transform(input_data_reshaped)\n",
        "print(std_data)"
      ],
      "metadata": {
        "id": "2CMrmn1Q9mX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now our input data is get stanadardized\n",
        "#hence we are rady to make prediction"
      ],
      "metadata": {
        "id": "APGf81dp-TNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = classifier.predict(std_data) #b/c we trained our ml model and stored in classifier,  and classifier has trained support vector machine model\n",
        "#print(prediction)"
      ],
      "metadata": {
        "id": "Jvqu_lhj-mWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(prediction[0]==0):\n",
        "  print('The person is non diabetic.')\n",
        "else:\n",
        "    print('The person is diabetic.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flchqUfH_FbX",
        "outputId": "c2de9588-8e88-4be2-9742-7743af65e696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The person is non diabetic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#why use prediction[0] --> b/c the prediction is a list & it has only one element(0 or 1),that why we use [0]."
      ],
      "metadata": {
        "id": "q5SDzR6o_huu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "                                                            **COMPLETED**"
      ],
      "metadata": {
        "id": "x-L07JCP_6d6"
      }
    }
  ]
}